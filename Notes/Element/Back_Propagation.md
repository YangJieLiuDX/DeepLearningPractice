# Back Propagation

Back propagation (backprop) allows the information from the cost to then flow backwards through the network, in order to compute the gradient.

## Overview

* Back-propagation is an algorithm that computes the *chain rule*, with a specific order of operations that is highly efficient.
* Back-propagation aka. chain rule is the procedure to compute gradients of the loss w.r.t. parameters in a multi-layer neural network. (to minimize a complicated function of the parameters)

## Resources

### Book

Deep Learning

* Ch 6.5 Back-Propagation and Other Differentiation Algorithm
    * Ch 6.5.1 Computational Graphs
    * Ch 6.5.2 Chain Rule of Calculus
    * Ch 6.5.4 Back-Propagation Computation in Fully-Connected MLP
    * Ch 6.5.7 Example: Back-Propagation for MLP Training