# Sequence-to-Sequence (Encoder-Decoder) Model

* **Encoder**: Input sequence --> Hidden states of input
* **Decoder**: Hidden state of output --> Output sequence

- Prediction of an output sequenc conditioned on an input sequence

Applications

* Machine translation
* Response generation in dialogue models
* Summarization
* Answer generation
* Paraphrase generation

## Neural Recurrent Sequence Models

## Recurrent Sequence to Sequence

## Convolutional-Based Sequence Models

## Transformer-based seq-to-seq

## Attention (The Encoder-Attention-Decoder) Architecture

**Attention**: "Pay attention to" different sub-sequences of the input when generating each of the token of the output

Modeling alignment in machine translation

## Bidirectional Encode Representations from Transformers (BERT)

Self-attention, borrowed from NMT, is powerful in representing contexts and responses

## Recources

### Tutorial

* [Andrew Ng - RNN W1L02: Notation (of sequence model)](https://youtu.be/XeQN82D4bCQ)

### Paper

* [Attention Is All You Need](https://arxiv.org/pdf/1706.03762.pdf)
* [BERT](https://arxiv.org/abs/1810.04805)

### Github

* [Google Research - BERT](https://github.com/google-research/bert)
