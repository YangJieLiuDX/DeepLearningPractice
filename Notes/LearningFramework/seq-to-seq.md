# Sequence-to-Sequence Model

## Neural Recurrent Sequence Models

## Recurrent Sequence to Sequence

## Convolutional-Based Sequence Models

## Transformer-based seq-to-seq

## Attention

## Bidirectional Encode Representations from Transformers (BERT)

## Recources

### Paper

* [Attention Is All You Need](https://arxiv.org/pdf/1706.03762.pdf)
* [BERT](https://arxiv.org/abs/1810.04805)

### Github

* [Google Research - BERT](https://github.com/google-research/bert)
