# [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805)

Quick notes

* BERT = Bidirectional Encoder Representations from Transformers.
* Can be fine-tuned with just one additional output layer
    * question answering
    * language inference

TBD

## Links

### Tutorial

* [TDLS: BERT, Pretranied Deep Bidirectional Transformers for Language Understanding (algorithm)](https://youtu.be/BhlOGGzC0Q0) - TODO
